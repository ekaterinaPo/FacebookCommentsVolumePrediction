{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comment Volume Prediction using Neural Networks and Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ekaterina Pogrebnyakova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning we import data set and external module with utility functions and other code in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sun\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import ep\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseDir = \"C:/Users/Sun/Desktop/UofT/3253/Final Project/Dataset\"\n",
    "trainFile1 = os.path.join(baseDir, \"Training\", \"Features_Variant_1.csv\")\n",
    "trainFile2 = os.path.join(baseDir, \"Training\", \"Features_Variant_2.csv\")\n",
    "trainFile3 = os.path.join(baseDir, \"Training\", \"Features_Variant_3.csv\")\n",
    "trainFile4 = os.path.join(baseDir, \"Training\", \"Features_Variant_4.csv\")\n",
    "trainFile5 = os.path.join(baseDir, \"Training\", \"Features_Variant_5.csv\")\n",
    "testFile1 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_1.csv\")\n",
    "testFile2 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_2.csv\")\n",
    "testFile3 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_3.csv\")\n",
    "testFile4 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_4.csv\")\n",
    "testFile5 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_5.csv\")\n",
    "testFile6 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_6.csv\")\n",
    "testFile7 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_7.csv\")\n",
    "testFile8 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_8.csv\")\n",
    "testFile9 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_9.csv\")\n",
    "testFile10 = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can start from Lasso and decision Tree Regressors. Also we use RandomizeSearchSV on hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor\n",
      "Training set: 1, test set: 1, time 15.18267560005188\n",
      "Training set: 1, test set: 2, time 17.293338298797607\n",
      "Training set: 1, test set: 3, time 17.40504813194275\n",
      "Training set: 1, test set: 4, time 16.93216586112976\n",
      "Training set: 1, test set: 5, time 15.681071043014526\n",
      "Training set: 1, test set: 6, time 17.802337408065796\n",
      "Training set: 1, test set: 7, time 21.028737545013428\n",
      "Training set: 1, test set: 8, time 15.099181413650513\n",
      "Training set: 1, test set: 9, time 15.333420753479004\n",
      "Training set: 1, test set: 10, time 14.832432985305786\n",
      "Training set: 2, test set: 1, time 25.97110915184021\n",
      "Training set: 2, test set: 2, time 30.430747985839844\n",
      "Training set: 2, test set: 3, time 31.313257932662964\n",
      "Training set: 2, test set: 4, time 28.47432255744934\n",
      "Training set: 2, test set: 5, time 32.625489950180054\n",
      "Training set: 2, test set: 6, time 27.886959552764893\n",
      "Training set: 2, test set: 7, time 30.99550461769104\n",
      "Training set: 2, test set: 8, time 28.969874382019043\n",
      "Training set: 2, test set: 9, time 25.668846130371094\n",
      "Training set: 2, test set: 10, time 28.67759943008423\n",
      "Training set: 3, test set: 1, time 38.93199396133423\n",
      "Training set: 3, test set: 2, time 37.02062201499939\n",
      "Training set: 3, test set: 3, time 37.982630252838135\n",
      "Training set: 3, test set: 4, time 36.91940712928772\n",
      "Training set: 3, test set: 5, time 39.222766160964966\n",
      "Training set: 3, test set: 6, time 38.40927267074585\n",
      "Training set: 3, test set: 7, time 40.13651657104492\n",
      "Training set: 3, test set: 8, time 36.463648080825806\n",
      "Training set: 3, test set: 9, time 38.631742000579834\n",
      "Training set: 3, test set: 10, time 38.38841652870178\n",
      "Training set: 4, test set: 1, time 46.1485869884491\n",
      "Training set: 4, test set: 2, time 48.289952516555786\n",
      "Training set: 4, test set: 3, time 49.39509296417236\n",
      "Training set: 4, test set: 4, time 47.68027400970459\n",
      "Training set: 4, test set: 5, time 44.868947982788086\n",
      "Training set: 4, test set: 6, time 48.12458682060242\n",
      "Training set: 4, test set: 7, time 46.97099566459656\n",
      "Training set: 4, test set: 8, time 45.037445306777954\n",
      "Training set: 4, test set: 9, time 45.49835729598999\n",
      "Training set: 4, test set: 10, time 48.76602053642273\n",
      "Training set: 5, test set: 1, time 58.89411520957947\n",
      "Training set: 5, test set: 2, time 58.162378549575806\n",
      "Training set: 5, test set: 3, time 49.90686869621277\n",
      "Training set: 5, test set: 4, time 57.65117383003235\n",
      "Training set: 5, test set: 5, time 57.06789755821228\n",
      "Training set: 5, test set: 6, time 53.03339385986328\n",
      "Training set: 5, test set: 7, time 54.92680859565735\n",
      "Training set: 5, test set: 8, time 62.25324630737305\n",
      "Training set: 5, test set: 9, time 55.85566520690918\n",
      "Training set: 5, test set: 10, time 54.81637215614319\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "cols = ['Regressor','Training Set','Test Set','Best Parameters', 'Time Train', 'HITS@10', 'AUC@10','MAE']\n",
    "df_scores = pd.DataFrame(columns = cols)\n",
    "\n",
    "lasso_param =  {'alpha': [0.5,1,2,5,10],\n",
    "               'fit_intercept': [True, False]}\n",
    "\n",
    "tree_param = {'max_depth': np.arange(3, 10),\n",
    "             'min_samples_split': [25, 50, 75, 100,150],\n",
    "             'min_samples_leaf':[25, 50, 75, 100,150],\n",
    "             'max_leaf_nodes':[25,50,75,100,150]}\n",
    "\n",
    "reg_name = ['Lasso Regression', 'Decision Tree Regressor']\n",
    "regressors =[linear_model.Lasso(), tree.DecisionTreeRegressor()]\n",
    "params = [lasso_param, tree_param]\n",
    "\n",
    "for k in range(len(regressors)):\n",
    "    print(reg_name[k]) \n",
    "    for i in range(1,6):\n",
    "        train_file = os.path.join(baseDir, \"Training\", \"Features_Variant_\" + str(i) + \".csv\")\n",
    "        for j in range(1,11):\n",
    "            scal = StandardScaler()\n",
    "            test_file = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_\" + str(j) + \".csv\")\n",
    "        # train\n",
    "            x_train, y_train = ep.GetXYsFromFile(train_file)\n",
    "            x_test, y_test = ep.GetXYsFromFile(test_file)\n",
    "            x_train = scal.fit_transform(x_train)\n",
    "            x_test = scal.transform(x_test)\n",
    "            t0 = time.time()\n",
    "           \n",
    "            grid = False\n",
    "            best_param, y_pred = ep.GetBestParams_train(regressors[k], params[k], x_train, y_train, x_test, y_test, grid = grid)\n",
    "            t1 = time.time()\n",
    "        # HITS@10\n",
    "            y_pred_10, y_test_10 = ep.set_10(y_test,y_pred)\n",
    "            dif = ep.hits_10(y_pred_10, y_test_10)\n",
    "            #print(\"Training set: %i, test set: %i, HITS@10: %s\" % (i, j, \"{:.1f}\".format(dif))) #y_10))\n",
    "        # AUC@10\n",
    "            AUC_10 = ep.AUC(y_pred_10, y_test_10)\n",
    "            #print(\"Training set: %i, test set: %i, AUC@10: %f\" % (i, j, AUC_10))\n",
    "        # Calculate MAE\n",
    "            MAE = mean_absolute_error(y_test, y_pred)\n",
    "            #print(\"Training set: %i, test set: %i, MAE: %f\" % (i, j, MAE))\n",
    "        # time taken\n",
    "            time_dif = (t1 - t0)\n",
    "            print(\"Training set: %i, test set: %i, time %s\" % (i, j, time_dif))\n",
    "        #make dataFrame with results\n",
    "            dict_score = pd.Series({ \n",
    "                     'Regressor': reg_name[k],\n",
    "                     'Training Set' : i,\n",
    "                     'Test Set': j,\n",
    "                     'Best Parameters':  best_param,\n",
    "                     'Time Train': time_dif,\n",
    "                     'HITS@10': dif,\n",
    "                     'AUC@10': AUC_10,\n",
    "                     'MAE': MAE},\n",
    "                )\n",
    "            \n",
    "            df_scores = df_scores.append(dict_score, ignore_index = True)\n",
    "# save output to file\n",
    "nowStr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "df_scores.to_csv(os.path.join(baseDir, \"%s-output-%s.csv\" % ('-'.join(reg_name), nowStr)))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_scores.to_csv(os.path.join(baseDir, \"lasso-decision-output-20180408-2149.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble of Decisiion Trees and instead of searching for the very best feature it searches for the best feature among a random subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1, test set: 1, time 6.350122928619385\n",
      "Training set: 1, test set: 2, time 5.503917217254639\n",
      "Training set: 1, test set: 3, time 5.704143524169922\n",
      "Training set: 1, test set: 4, time 5.371216773986816\n",
      "Training set: 1, test set: 5, time 6.095407962799072\n",
      "Training set: 1, test set: 6, time 5.63887095451355\n",
      "Training set: 1, test set: 7, time 5.614458084106445\n",
      "Training set: 1, test set: 8, time 5.349130868911743\n",
      "Training set: 1, test set: 9, time 5.85679030418396\n",
      "Training set: 1, test set: 10, time 5.413005113601685\n",
      "Training set: 2, test set: 1, time 11.975838661193848\n",
      "Training set: 2, test set: 2, time 10.962160348892212\n",
      "Training set: 2, test set: 3, time 10.545807123184204\n",
      "Training set: 2, test set: 4, time 12.056237936019897\n",
      "Training set: 2, test set: 5, time 11.782790660858154\n",
      "Training set: 2, test set: 6, time 12.13387417793274\n",
      "Training set: 2, test set: 7, time 9.769396543502808\n",
      "Training set: 2, test set: 8, time 11.407205820083618\n",
      "Training set: 2, test set: 9, time 11.314543962478638\n",
      "Training set: 2, test set: 10, time 12.26926875114441\n",
      "Training set: 3, test set: 1, time 21.164472341537476\n",
      "Training set: 3, test set: 2, time 19.032923698425293\n",
      "Training set: 3, test set: 3, time 17.910080194473267\n",
      "Training set: 3, test set: 4, time 17.637717485427856\n",
      "Training set: 3, test set: 5, time 17.461865663528442\n",
      "Training set: 3, test set: 6, time 16.999303817749023\n",
      "Training set: 3, test set: 7, time 17.76290464401245\n",
      "Training set: 3, test set: 8, time 18.47333550453186\n",
      "Training set: 3, test set: 9, time 18.760322332382202\n",
      "Training set: 3, test set: 10, time 23.599872589111328\n",
      "Training set: 4, test set: 1, time 32.25651526451111\n",
      "Training set: 4, test set: 2, time 34.0298216342926\n",
      "Training set: 4, test set: 3, time 29.323852062225342\n",
      "Training set: 4, test set: 4, time 25.49888277053833\n",
      "Training set: 4, test set: 5, time 25.474400758743286\n",
      "Training set: 4, test set: 6, time 25.028745651245117\n",
      "Training set: 4, test set: 7, time 28.85225796699524\n",
      "Training set: 4, test set: 8, time 23.41976022720337\n",
      "Training set: 4, test set: 9, time 22.931142568588257\n",
      "Training set: 4, test set: 10, time 27.287721633911133\n",
      "Training set: 5, test set: 1, time 29.790052890777588\n",
      "Training set: 5, test set: 2, time 27.672442197799683\n",
      "Training set: 5, test set: 3, time 27.974787950515747\n",
      "Training set: 5, test set: 4, time 29.16094946861267\n",
      "Training set: 5, test set: 5, time 30.757732391357422\n",
      "Training set: 5, test set: 6, time 32.421679973602295\n",
      "Training set: 5, test set: 7, time 34.02495837211609\n",
      "Training set: 5, test set: 8, time 32.651010274887085\n",
      "Training set: 5, test set: 9, time 33.23050022125244\n",
      "Training set: 5, test set: 10, time 36.105560064315796\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "cols = ['Regressor','Training Set','Test Set','Best Parameters', 'Time Train', 'HITS@10', 'AUC@10','MAE']\n",
    "df_scores_rf = pd.DataFrame(columns = cols)\n",
    "reg_name = 'RF'\n",
    " \n",
    "for i in range(1,6):\n",
    "    train_file = os.path.join(baseDir, \"Training\", \"Features_Variant_\" + str(i) + \".csv\")\n",
    "    for j in range(1,11):\n",
    "        scal = StandardScaler()\n",
    "        test_file = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_\" + str(j) + \".csv\")\n",
    "    # train\n",
    "        x_train, y_train = ep.GetXYsFromFile(train_file)\n",
    "        x_test, y_test = ep.GetXYsFromFile(test_file)\n",
    "        x_train = scal.fit_transform(x_train)\n",
    "        x_test = scal.transform(x_test)\n",
    "        t0 = time.time()\n",
    "        regr = RandomForestRegressor(n_estimators = 25, max_depth = 9, min_samples_split = 50, min_samples_leaf = 50, n_jobs = -1)\n",
    "        regr.fit(x_train, y_train)\n",
    "        y_pred = regr.predict(x_test)\n",
    "        t1 = time.time()\n",
    "    # HITS@10\n",
    "        y_pred_10, y_test_10 = ep.set_10(y_test,y_pred)\n",
    "        dif = ep.hits_10(y_pred_10, y_test_10)\n",
    "        #print(\"Training set: %i, test set: %i, HITS@10: %s\" % (i, j, \"{:.1f}\".format(dif))) #y_10))\n",
    "    # AUC@10\n",
    "        AUC_10 = ep.AUC(y_pred_10, y_test_10)\n",
    "        #print(\"Training set: %i, test set: %i, AUC@10: %f\" % (i, j, AUC_10))\n",
    "    # Calculate MAE\n",
    "        MAE = mean_absolute_error(y_test, y_pred)\n",
    "        #print(\"Training set: %i, test set: %i, MAE: %f\" % (i, j, MAE))\n",
    "    # time taken\n",
    "        time_dif = (t1 - t0)\n",
    "        print(\"Training set: %i, test set: %i, time %s\" % (i, j, time_dif))\n",
    "    #make dataFrame with results\n",
    "        dict_score_rf = pd.Series({ \n",
    "                 'Regressor': 'Random Forest Regressor',\n",
    "                 'Training Set' : i,\n",
    "                 'Test Set': j,\n",
    "                 'Best Parameters':  'n_estimators = 25, max_depth = 9, min_samples_split = 50, min_samples_leaf = 50',\n",
    "                 'Time Train': time_dif,\n",
    "                 'HITS@10': dif,\n",
    "                 'AUC@10': AUC_10,\n",
    "                 'MAE': MAE},\n",
    "            )\n",
    "\n",
    "        df_scores_rf = df_scores_rf.append(dict_score_rf, ignore_index = True)\n",
    "# save output to file\n",
    "nowStr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "df_scores_rf.to_csv(os.path.join(baseDir, \"%s-output-%s.csv\" % ('-'.join(reg_name), nowStr)))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network - Multi-Layer Perceptrons (MLP) is used in 2 forms:  \n",
    "* single hidden layer with 4 neurons  \n",
    "* two hidden layers: 20 neurons in the first and 4 in the second hidden layer.\n",
    "\n",
    "For both of these cases the learning rate is fixed to 0.01 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 53\n",
    "n_hidden1 = 20\n",
    "n_hidden2 = 4\n",
    "n_outputs = 1\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # 1 hidden layer\n",
    "    hidden2 = tf.layers.dense(X, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    regress = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.losses.absolute_difference(y, tf.squeeze(regress))  # absolute error\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean loss 6.41\n",
      "Epoch 1, mean loss 5.52\n",
      "Epoch 2, mean loss 5.31\n",
      "Epoch 3, mean loss 4.96\n",
      "Epoch 4, mean loss 5.23\n",
      "1 MAE: 22.394041581857817\n",
      "Epoch 0, mean loss 5.17\n",
      "Epoch 1, mean loss 5.06\n",
      "Epoch 2, mean loss 5.12\n",
      "Epoch 3, mean loss 4.90\n",
      "Epoch 4, mean loss 4.98\n",
      "1 MAE: 16.33320236195001\n",
      "Epoch 0, mean loss 4.93\n",
      "Epoch 1, mean loss 4.95\n",
      "Epoch 2, mean loss 5.03\n",
      "Epoch 3, mean loss 5.04\n",
      "Epoch 4, mean loss 4.78\n",
      "1 MAE: 17.49332317529303\n",
      "Epoch 0, mean loss 4.94\n",
      "Epoch 1, mean loss 4.81\n",
      "Epoch 2, mean loss 4.82\n",
      "Epoch 3, mean loss 4.83\n",
      "Epoch 4, mean loss 4.96\n",
      "1 MAE: 28.835893767225002\n",
      "Epoch 0, mean loss 4.72\n",
      "Epoch 1, mean loss 4.64\n",
      "Epoch 2, mean loss 4.75\n",
      "Epoch 3, mean loss 4.60\n",
      "Epoch 4, mean loss 4.88\n",
      "1 MAE: 15.276971092485267\n",
      "Epoch 0, mean loss 4.72\n",
      "Epoch 1, mean loss 4.78\n",
      "Epoch 2, mean loss 4.63\n",
      "Epoch 3, mean loss 4.80\n",
      "Epoch 4, mean loss 4.63\n",
      "1 MAE: 10.001048213099082\n",
      "Epoch 0, mean loss 4.63\n",
      "Epoch 1, mean loss 4.62\n",
      "Epoch 2, mean loss 4.85\n",
      "Epoch 3, mean loss 4.62\n",
      "Epoch 4, mean loss 4.79\n",
      "1 MAE: 18.43064895126912\n",
      "Epoch 0, mean loss 4.54\n",
      "Epoch 1, mean loss 4.68\n",
      "Epoch 2, mean loss 4.57\n",
      "Epoch 3, mean loss 4.70\n",
      "Epoch 4, mean loss 4.51\n",
      "1 MAE: 36.789048301749816\n",
      "Epoch 0, mean loss 4.70\n",
      "Epoch 1, mean loss 4.74\n",
      "Epoch 2, mean loss 4.81\n",
      "Epoch 3, mean loss 4.65\n",
      "Epoch 4, mean loss 4.69\n",
      "1 MAE: 21.50510580282696\n",
      "Epoch 0, mean loss 4.41\n",
      "Epoch 1, mean loss 4.48\n",
      "Epoch 2, mean loss 4.62\n",
      "Epoch 3, mean loss 4.75\n",
      "Epoch 4, mean loss 4.58\n",
      "1 MAE: 27.347555939515\n",
      "Epoch 0, mean loss 4.47\n",
      "Epoch 1, mean loss 4.59\n",
      "Epoch 2, mean loss 4.56\n",
      "Epoch 3, mean loss 4.22\n",
      "Epoch 4, mean loss 4.53\n",
      "2 MAE: 24.237121123316076\n",
      "Epoch 0, mean loss 4.41\n",
      "Epoch 1, mean loss 4.56\n",
      "Epoch 2, mean loss 4.55\n",
      "Epoch 3, mean loss 4.48\n",
      "Epoch 4, mean loss 4.47\n",
      "2 MAE: 16.350599646060303\n",
      "Epoch 0, mean loss 4.46\n",
      "Epoch 1, mean loss 4.35\n",
      "Epoch 2, mean loss 4.60\n",
      "Epoch 3, mean loss 4.30\n",
      "Epoch 4, mean loss 4.30\n",
      "2 MAE: 14.02725102432599\n",
      "Epoch 0, mean loss 4.35\n",
      "Epoch 1, mean loss 4.37\n",
      "Epoch 2, mean loss 4.41\n",
      "Epoch 3, mean loss 4.43\n",
      "Epoch 4, mean loss 4.25\n",
      "2 MAE: 30.031889468527897\n",
      "Epoch 0, mean loss 4.32\n",
      "Epoch 1, mean loss 4.28\n",
      "Epoch 2, mean loss 4.27\n",
      "Epoch 3, mean loss 4.42\n",
      "Epoch 4, mean loss 4.53\n",
      "2 MAE: 19.301914795720478\n",
      "Epoch 0, mean loss 4.17\n",
      "Epoch 1, mean loss 4.50\n",
      "Epoch 2, mean loss 4.31\n",
      "Epoch 3, mean loss 4.22\n",
      "Epoch 4, mean loss 4.39\n",
      "2 MAE: 9.902008773829563\n",
      "Epoch 0, mean loss 4.47\n",
      "Epoch 1, mean loss 4.26\n",
      "Epoch 2, mean loss 4.21\n",
      "Epoch 3, mean loss 4.52\n",
      "Epoch 4, mean loss 4.39\n",
      "2 MAE: 16.49049786630209\n",
      "Epoch 0, mean loss 4.44\n",
      "Epoch 1, mean loss 4.26\n",
      "Epoch 2, mean loss 4.18\n",
      "Epoch 3, mean loss 4.37\n",
      "Epoch 4, mean loss 4.38\n",
      "2 MAE: 37.772218843160026\n",
      "Epoch 0, mean loss 4.31\n",
      "Epoch 1, mean loss 4.24\n",
      "Epoch 2, mean loss 4.22\n",
      "Epoch 3, mean loss 4.42\n",
      "Epoch 4, mean loss 4.38\n",
      "2 MAE: 22.042331203282107\n",
      "Epoch 0, mean loss 4.39\n",
      "Epoch 1, mean loss 4.38\n",
      "Epoch 2, mean loss 4.32\n",
      "Epoch 3, mean loss 4.19\n",
      "Epoch 4, mean loss 4.25\n",
      "2 MAE: 24.098293765583264\n",
      "Epoch 0, mean loss 4.24\n",
      "Epoch 1, mean loss 4.21\n",
      "Epoch 2, mean loss 4.23\n",
      "Epoch 3, mean loss 4.23\n",
      "Epoch 4, mean loss 4.20\n",
      "3 MAE: 25.40272448769761\n",
      "Epoch 0, mean loss 4.32\n",
      "Epoch 1, mean loss 4.08\n",
      "Epoch 2, mean loss 4.20\n",
      "Epoch 3, mean loss 4.19\n",
      "Epoch 4, mean loss 4.18\n",
      "3 MAE: 16.578443620198716\n",
      "Epoch 0, mean loss 4.17\n",
      "Epoch 1, mean loss 4.25\n",
      "Epoch 2, mean loss 4.20\n",
      "Epoch 3, mean loss 4.11\n",
      "Epoch 4, mean loss 4.10\n",
      "3 MAE: 13.670669555181938\n",
      "Epoch 0, mean loss 4.16\n",
      "Epoch 1, mean loss 4.14\n",
      "Epoch 2, mean loss 4.13\n",
      "Epoch 3, mean loss 4.22\n",
      "Epoch 4, mean loss 4.16\n",
      "3 MAE: 30.48472785591289\n",
      "Epoch 0, mean loss 4.00\n",
      "Epoch 1, mean loss 4.08\n",
      "Epoch 2, mean loss 4.04\n",
      "Epoch 3, mean loss 4.03\n",
      "Epoch 4, mean loss 4.13\n",
      "3 MAE: 20.006692721113335\n",
      "Epoch 0, mean loss 3.97\n",
      "Epoch 1, mean loss 3.93\n",
      "Epoch 2, mean loss 3.99\n",
      "Epoch 3, mean loss 3.89\n",
      "Epoch 4, mean loss 4.02\n",
      "3 MAE: 9.058043605223476\n",
      "Epoch 0, mean loss 4.07\n",
      "Epoch 1, mean loss 4.07\n",
      "Epoch 2, mean loss 3.96\n",
      "Epoch 3, mean loss 3.97\n",
      "Epoch 4, mean loss 3.98\n",
      "3 MAE: 16.6623532157015\n",
      "Epoch 0, mean loss 4.00\n",
      "Epoch 1, mean loss 3.96\n",
      "Epoch 2, mean loss 3.92\n",
      "Epoch 3, mean loss 4.03\n",
      "Epoch 4, mean loss 4.09\n",
      "3 MAE: 35.300695724098595\n",
      "Epoch 0, mean loss 4.08\n",
      "Epoch 1, mean loss 3.94\n",
      "Epoch 2, mean loss 3.97\n",
      "Epoch 3, mean loss 3.95\n",
      "Epoch 4, mean loss 3.99\n",
      "3 MAE: 19.01808764689221\n",
      "Epoch 0, mean loss 3.90\n",
      "Epoch 1, mean loss 3.98\n",
      "Epoch 2, mean loss 4.13\n",
      "Epoch 3, mean loss 3.93\n",
      "Epoch 4, mean loss 3.95\n",
      "3 MAE: 23.33146796897618\n",
      "Epoch 0, mean loss 4.16\n",
      "Epoch 1, mean loss 4.06\n",
      "Epoch 2, mean loss 4.16\n",
      "Epoch 3, mean loss 4.04\n",
      "Epoch 4, mean loss 4.03\n",
      "4 MAE: 23.144227816060074\n",
      "Epoch 0, mean loss 4.04\n",
      "Epoch 1, mean loss 4.15\n",
      "Epoch 2, mean loss 3.94\n",
      "Epoch 3, mean loss 3.96\n",
      "Epoch 4, mean loss 4.00\n",
      "4 MAE: 14.710195485709441\n",
      "Epoch 0, mean loss 4.09\n",
      "Epoch 1, mean loss 4.10\n",
      "Epoch 2, mean loss 4.05\n",
      "Epoch 3, mean loss 4.03\n",
      "Epoch 4, mean loss 4.03\n",
      "4 MAE: 41.15864623093395\n",
      "Epoch 0, mean loss 4.13\n",
      "Epoch 1, mean loss 4.06\n",
      "Epoch 2, mean loss 4.00\n",
      "Epoch 3, mean loss 4.00\n",
      "Epoch 4, mean loss 3.99\n",
      "4 MAE: 28.889832567762245\n",
      "Epoch 0, mean loss 4.06\n",
      "Epoch 1, mean loss 4.13\n",
      "Epoch 2, mean loss 3.96\n",
      "Epoch 3, mean loss 3.93\n",
      "Epoch 4, mean loss 4.04\n",
      "4 MAE: 15.712710805383105\n",
      "Epoch 0, mean loss 3.99\n",
      "Epoch 1, mean loss 3.99\n",
      "Epoch 2, mean loss 3.97\n",
      "Epoch 3, mean loss 4.01\n",
      "Epoch 4, mean loss 4.04\n",
      "4 MAE: 8.300180991095575\n",
      "Epoch 0, mean loss 4.00\n",
      "Epoch 1, mean loss 3.94\n",
      "Epoch 2, mean loss 4.04\n",
      "Epoch 3, mean loss 3.94\n",
      "Epoch 4, mean loss 4.06\n",
      "4 MAE: 16.275489009953237\n",
      "Epoch 0, mean loss 3.92\n",
      "Epoch 1, mean loss 4.08\n",
      "Epoch 2, mean loss 3.95\n",
      "Epoch 3, mean loss 3.89\n",
      "Epoch 4, mean loss 4.04\n",
      "4 MAE: 38.589125496735136\n",
      "Epoch 0, mean loss 4.00\n",
      "Epoch 1, mean loss 3.93\n",
      "Epoch 2, mean loss 4.03\n",
      "Epoch 3, mean loss 3.98\n",
      "Epoch 4, mean loss 3.93\n",
      "4 MAE: 21.309850942178844\n",
      "Epoch 0, mean loss 3.89\n",
      "Epoch 1, mean loss 4.04\n",
      "Epoch 2, mean loss 4.11\n",
      "Epoch 3, mean loss 3.96\n",
      "Epoch 4, mean loss 3.94\n",
      "4 MAE: 28.228998737063993\n",
      "Epoch 0, mean loss 4.07\n",
      "Epoch 1, mean loss 4.03\n",
      "Epoch 2, mean loss 4.13\n",
      "Epoch 3, mean loss 4.05\n",
      "Epoch 4, mean loss 4.04\n",
      "5 MAE: 22.82355288079366\n",
      "Epoch 0, mean loss 3.98\n",
      "Epoch 1, mean loss 4.01\n",
      "Epoch 2, mean loss 3.97\n",
      "Epoch 3, mean loss 3.99\n",
      "Epoch 4, mean loss 4.11\n",
      "5 MAE: 14.403426719189744\n",
      "Epoch 0, mean loss 4.02\n",
      "Epoch 1, mean loss 3.99\n",
      "Epoch 2, mean loss 4.01\n",
      "Epoch 3, mean loss 3.92\n",
      "Epoch 4, mean loss 4.01\n",
      "5 MAE: 15.327544807298862\n",
      "Epoch 0, mean loss 4.13\n",
      "Epoch 1, mean loss 4.02\n",
      "Epoch 2, mean loss 3.99\n",
      "Epoch 3, mean loss 3.98\n",
      "Epoch 4, mean loss 4.04\n",
      "5 MAE: 28.356784285507118\n",
      "Epoch 0, mean loss 4.08\n",
      "Epoch 1, mean loss 3.99\n",
      "Epoch 2, mean loss 3.96\n",
      "Epoch 3, mean loss 3.99\n",
      "Epoch 4, mean loss 3.97\n",
      "5 MAE: 22.11888216046447\n",
      "Epoch 0, mean loss 4.00\n",
      "Epoch 1, mean loss 4.02\n",
      "Epoch 2, mean loss 3.96\n",
      "Epoch 3, mean loss 4.05\n",
      "Epoch 4, mean loss 4.00\n",
      "5 MAE: 6.79114563357687\n",
      "Epoch 0, mean loss 3.99\n",
      "Epoch 1, mean loss 3.93\n",
      "Epoch 2, mean loss 4.03\n",
      "Epoch 3, mean loss 3.97\n",
      "Epoch 4, mean loss 3.96\n",
      "5 MAE: 16.818743279424844\n",
      "Epoch 0, mean loss 4.01\n",
      "Epoch 1, mean loss 4.03\n",
      "Epoch 2, mean loss 3.94\n",
      "Epoch 3, mean loss 3.96\n",
      "Epoch 4, mean loss 4.02\n",
      "5 MAE: 38.872293385158464\n",
      "Epoch 0, mean loss 3.92\n",
      "Epoch 1, mean loss 3.94\n",
      "Epoch 2, mean loss 3.90\n",
      "Epoch 3, mean loss 4.00\n",
      "Epoch 4, mean loss 4.02\n",
      "5 MAE: 24.163559439206363\n",
      "Epoch 0, mean loss 3.99\n",
      "Epoch 1, mean loss 3.98\n",
      "Epoch 2, mean loss 4.03\n",
      "Epoch 3, mean loss 3.87\n",
      "Epoch 4, mean loss 4.08\n",
      "5 MAE: 22.53989352383698\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "df_NN_1 = pd.DataFrame(columns = cols)\n",
    "NN_name = 'MLP4'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        train_file = os.path.join(baseDir, \"Training\", \"Features_Variant_\" + str(i) + \".csv\")\n",
    "        for j in range(1, 11):\n",
    "            t0_ = time.time()\n",
    "            test_file = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_\" + str(j) + \".csv\")\n",
    "\n",
    "            x_train, y_train = ep.GetXYsFromFile(train_file)\n",
    "            x_test, y_test = ep.GetXYsFromFile(test_file)\n",
    "            scal = StandardScaler()\n",
    "            x_train = pd.DataFrame(scal.fit_transform(x_train))\n",
    "            x_test = pd.DataFrame(scal.transform(x_test))\n",
    "            \n",
    "            losses = []\n",
    "            n_epochs = 5\n",
    "            batch_size = 500\n",
    "            n_batches = len(x_train) // batch_size\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                for iteration in range(n_batches):\n",
    "                    X_batch, y_batch = ep.fetch_batch(x_train, y_train, epoch, iteration, batch_size)\n",
    "                    _, l, reg = sess.run([training_op, loss, regress], feed_dict={X: X_batch, y: y_batch})\n",
    "                    losses.append(l)\n",
    "                print(\"Epoch %i, mean loss %.2f\" % (epoch, np.mean(losses[-n_batches:])))\n",
    "\n",
    "            yPred = sess.run(regress, feed_dict = { X: x_test })\n",
    "            \n",
    "            # metrics\n",
    "            mae_val = mean_absolute_error(y_test, yPred)\n",
    "            print(i, \"MAE:\", mae_val)\n",
    "            y_pred_10, y_test_10 = ep.set_10(y_test,yPred)\n",
    "            dif = ep.hits_10(y_pred_10, y_test_10)\n",
    "            \n",
    "            AUC_10_ = ep.AUC(y_pred_10, y_test_10)\n",
    "            t1_ = time.time()\n",
    "            time_dif_ = t1_ - t0_\n",
    "            #print(dif_)\n",
    "            NN_score = pd.Series({ \n",
    "                     'Regressor': 'MLP-4',\n",
    "                     'Training Set' : i,\n",
    "                     'Test Set': j,\n",
    "                     'Best Parameters':  'hidden layer:4',\n",
    "                     'Time Train': time_dif_,\n",
    "                     'HITS@10': dif,\n",
    "                     'AUC@10': AUC_10_,\n",
    "                     'MAE': mae_val},\n",
    "                )\n",
    "            \n",
    "            df_NN_1 = df_NN_1.append(NN_score, ignore_index = True)\n",
    "            \n",
    "# save output to file\n",
    "nowStr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "df_NN_1.to_csv(os.path.join(baseDir, \"%s-output-%s.csv\" % ('-'.join(NN_name), nowStr)))\n",
    "\n",
    "print(\"Done.\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # 1 hidden layer\n",
    "    hidden2 = tf.layers.dense(X, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    regress = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.losses.absolute_difference(y, tf.squeeze(regress))  # absolute error\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean loss 6.39\n",
      "Epoch 1, mean loss 5.23\n",
      "Epoch 2, mean loss 5.12\n",
      "Epoch 3, mean loss 5.14\n",
      "Epoch 4, mean loss 4.98\n",
      "1 MAE: 22.340634809731686\n",
      "Epoch 0, mean loss 5.16\n",
      "Epoch 1, mean loss 4.86\n",
      "Epoch 2, mean loss 5.01\n",
      "Epoch 3, mean loss 4.77\n",
      "Epoch 4, mean loss 5.33\n",
      "1 MAE: 16.163521938925523\n",
      "Epoch 0, mean loss 4.85\n",
      "Epoch 1, mean loss 4.73\n",
      "Epoch 2, mean loss 4.76\n",
      "Epoch 3, mean loss 4.87\n",
      "Epoch 4, mean loss 4.86\n",
      "1 MAE: 15.291038057080122\n",
      "Epoch 0, mean loss 4.91\n",
      "Epoch 1, mean loss 4.97\n",
      "Epoch 2, mean loss 4.69\n",
      "Epoch 3, mean loss 4.79\n",
      "Epoch 4, mean loss 4.66\n",
      "1 MAE: 29.097777724022638\n",
      "Epoch 0, mean loss 4.92\n",
      "Epoch 1, mean loss 4.95\n",
      "Epoch 2, mean loss 4.75\n",
      "Epoch 3, mean loss 4.76\n",
      "Epoch 4, mean loss 4.81\n",
      "1 MAE: 15.430274648046252\n",
      "Epoch 0, mean loss 4.86\n",
      "Epoch 1, mean loss 4.52\n",
      "Epoch 2, mean loss 4.68\n",
      "Epoch 3, mean loss 4.74\n",
      "Epoch 4, mean loss 4.80\n",
      "1 MAE: 9.888433229908431\n",
      "Epoch 0, mean loss 4.72\n",
      "Epoch 1, mean loss 4.89\n",
      "Epoch 2, mean loss 4.63\n",
      "Epoch 3, mean loss 4.82\n",
      "Epoch 4, mean loss 4.54\n",
      "1 MAE: 17.19427957862023\n",
      "Epoch 0, mean loss 4.55\n",
      "Epoch 1, mean loss 4.48\n",
      "Epoch 2, mean loss 4.52\n",
      "Epoch 3, mean loss 4.81\n",
      "Epoch 4, mean loss 4.46\n",
      "1 MAE: 38.791804135567745\n",
      "Epoch 0, mean loss 4.61\n",
      "Epoch 1, mean loss 4.23\n",
      "Epoch 2, mean loss 4.75\n",
      "Epoch 3, mean loss 4.61\n",
      "Epoch 4, mean loss 4.62\n",
      "1 MAE: 21.877812239270884\n",
      "Epoch 0, mean loss 4.56\n",
      "Epoch 1, mean loss 4.74\n",
      "Epoch 2, mean loss 4.75\n",
      "Epoch 3, mean loss 4.73\n",
      "Epoch 4, mean loss 4.79\n",
      "1 MAE: 25.233294065603094\n",
      "Epoch 0, mean loss 4.49\n",
      "Epoch 1, mean loss 4.66\n",
      "Epoch 2, mean loss 4.49\n",
      "Epoch 3, mean loss 4.24\n",
      "Epoch 4, mean loss 4.47\n",
      "2 MAE: 24.514478462741174\n",
      "Epoch 0, mean loss 4.52\n",
      "Epoch 1, mean loss 4.36\n",
      "Epoch 2, mean loss 4.39\n",
      "Epoch 3, mean loss 4.45\n",
      "Epoch 4, mean loss 4.39\n",
      "2 MAE: 17.289381454662287\n",
      "Epoch 0, mean loss 4.36\n",
      "Epoch 1, mean loss 4.37\n",
      "Epoch 2, mean loss 4.59\n",
      "Epoch 3, mean loss 4.43\n",
      "Epoch 4, mean loss 4.53\n",
      "2 MAE: 14.863219823175571\n",
      "Epoch 0, mean loss 4.47\n",
      "Epoch 1, mean loss 4.32\n",
      "Epoch 2, mean loss 4.50\n",
      "Epoch 3, mean loss 4.32\n",
      "Epoch 4, mean loss 4.32\n",
      "2 MAE: 30.536368169687744\n",
      "Epoch 0, mean loss 4.48\n",
      "Epoch 1, mean loss 4.32\n",
      "Epoch 2, mean loss 4.34\n",
      "Epoch 3, mean loss 4.36\n",
      "Epoch 4, mean loss 4.33\n",
      "2 MAE: 16.67577389356706\n",
      "Epoch 0, mean loss 4.49\n",
      "Epoch 1, mean loss 4.39\n",
      "Epoch 2, mean loss 4.45\n",
      "Epoch 3, mean loss 4.54\n",
      "Epoch 4, mean loss 4.43\n",
      "2 MAE: 10.138479360770857\n",
      "Epoch 0, mean loss 4.24\n",
      "Epoch 1, mean loss 4.51\n",
      "Epoch 2, mean loss 4.39\n",
      "Epoch 3, mean loss 4.44\n",
      "Epoch 4, mean loss 4.34\n",
      "2 MAE: 17.713010374126213\n",
      "Epoch 0, mean loss 4.24\n",
      "Epoch 1, mean loss 4.56\n",
      "Epoch 2, mean loss 4.28\n",
      "Epoch 3, mean loss 4.47\n",
      "Epoch 4, mean loss 4.53\n",
      "2 MAE: 37.286915465105196\n",
      "Epoch 0, mean loss 4.48\n",
      "Epoch 1, mean loss 4.32\n",
      "Epoch 2, mean loss 4.38\n",
      "Epoch 3, mean loss 4.31\n",
      "Epoch 4, mean loss 4.31\n",
      "2 MAE: 19.117274331220546\n",
      "Epoch 0, mean loss 4.46\n",
      "Epoch 1, mean loss 4.36\n",
      "Epoch 2, mean loss 4.36\n",
      "Epoch 3, mean loss 4.37\n",
      "Epoch 4, mean loss 4.32\n",
      "2 MAE: 24.702695061697277\n",
      "Epoch 0, mean loss 4.24\n",
      "Epoch 1, mean loss 4.19\n",
      "Epoch 2, mean loss 4.25\n",
      "Epoch 3, mean loss 4.13\n",
      "Epoch 4, mean loss 4.07\n",
      "3 MAE: 25.049367543389888\n",
      "Epoch 0, mean loss 4.19\n",
      "Epoch 1, mean loss 4.13\n",
      "Epoch 2, mean loss 4.06\n",
      "Epoch 3, mean loss 4.18\n",
      "Epoch 4, mean loss 4.14\n",
      "3 MAE: 17.112703866668685\n",
      "Epoch 0, mean loss 4.24\n",
      "Epoch 1, mean loss 4.04\n",
      "Epoch 2, mean loss 4.13\n",
      "Epoch 3, mean loss 4.17\n",
      "Epoch 4, mean loss 4.16\n",
      "3 MAE: 12.438443620038933\n",
      "Epoch 0, mean loss 4.06\n",
      "Epoch 1, mean loss 4.16\n",
      "Epoch 2, mean loss 4.14\n",
      "Epoch 3, mean loss 4.18\n",
      "Epoch 4, mean loss 4.23\n",
      "3 MAE: 30.381480811349576\n",
      "Epoch 0, mean loss 4.15\n",
      "Epoch 1, mean loss 4.14\n",
      "Epoch 2, mean loss 4.13\n",
      "Epoch 3, mean loss 4.15\n",
      "Epoch 4, mean loss 4.21\n",
      "3 MAE: 15.307177642950638\n",
      "Epoch 0, mean loss 4.06\n",
      "Epoch 1, mean loss 4.23\n",
      "Epoch 2, mean loss 4.05\n",
      "Epoch 3, mean loss 4.22\n",
      "Epoch 4, mean loss 4.11\n",
      "3 MAE: 9.93895703372769\n",
      "Epoch 0, mean loss 4.17\n",
      "Epoch 1, mean loss 4.09\n",
      "Epoch 2, mean loss 4.13\n",
      "Epoch 3, mean loss 4.06\n",
      "Epoch 4, mean loss 4.02\n",
      "3 MAE: 18.563080582894724\n",
      "Epoch 0, mean loss 4.20\n",
      "Epoch 1, mean loss 4.16\n",
      "Epoch 2, mean loss 4.13\n",
      "Epoch 3, mean loss 4.00\n",
      "Epoch 4, mean loss 4.11\n",
      "3 MAE: 40.00709517775315\n",
      "Epoch 0, mean loss 4.10\n",
      "Epoch 1, mean loss 3.99\n",
      "Epoch 2, mean loss 4.07\n",
      "Epoch 3, mean loss 4.03\n",
      "Epoch 4, mean loss 4.17\n",
      "3 MAE: 17.91798221851779\n",
      "Epoch 0, mean loss 4.23\n",
      "Epoch 1, mean loss 4.04\n",
      "Epoch 2, mean loss 4.06\n",
      "Epoch 3, mean loss 3.97\n",
      "Epoch 4, mean loss 3.97\n",
      "3 MAE: 30.876469203877743\n",
      "Epoch 0, mean loss 4.27\n",
      "Epoch 1, mean loss 4.28\n",
      "Epoch 2, mean loss 4.12\n",
      "Epoch 3, mean loss 4.27\n",
      "Epoch 4, mean loss 4.17\n",
      "4 MAE: 26.629217684040825\n",
      "Epoch 0, mean loss 4.24\n",
      "Epoch 1, mean loss 4.13\n",
      "Epoch 2, mean loss 4.31\n",
      "Epoch 3, mean loss 4.01\n",
      "Epoch 4, mean loss 4.15\n",
      "4 MAE: 18.93767710768552\n",
      "Epoch 0, mean loss 4.24\n",
      "Epoch 1, mean loss 4.21\n",
      "Epoch 2, mean loss 4.20\n",
      "Epoch 3, mean loss 4.16\n",
      "Epoch 4, mean loss 4.10\n",
      "4 MAE: 53.34130473213853\n",
      "Epoch 0, mean loss 4.08\n",
      "Epoch 1, mean loss 4.25\n",
      "Epoch 2, mean loss 4.01\n",
      "Epoch 3, mean loss 4.13\n",
      "Epoch 4, mean loss 4.10\n",
      "4 MAE: 30.834914761788276\n",
      "Epoch 0, mean loss 4.03\n",
      "Epoch 1, mean loss 4.06\n",
      "Epoch 2, mean loss 4.11\n",
      "Epoch 3, mean loss 4.15\n",
      "Epoch 4, mean loss 4.15\n",
      "4 MAE: 30.682549420400814\n",
      "Epoch 0, mean loss 4.22\n",
      "Epoch 1, mean loss 4.07\n",
      "Epoch 2, mean loss 3.96\n",
      "Epoch 3, mean loss 4.03\n",
      "Epoch 4, mean loss 4.15\n",
      "4 MAE: 10.313672939287216\n",
      "Epoch 0, mean loss 4.20\n",
      "Epoch 1, mean loss 4.22\n",
      "Epoch 2, mean loss 4.12\n",
      "Epoch 3, mean loss 4.11\n",
      "Epoch 4, mean loss 4.14\n",
      "4 MAE: 16.508071332581395\n",
      "Epoch 0, mean loss 4.12\n",
      "Epoch 1, mean loss 4.11\n",
      "Epoch 2, mean loss 4.17\n",
      "Epoch 3, mean loss 4.07\n",
      "Epoch 4, mean loss 4.07\n",
      "4 MAE: 170.06091597471197\n",
      "Epoch 0, mean loss 4.13\n",
      "Epoch 1, mean loss 4.00\n",
      "Epoch 2, mean loss 4.06\n",
      "Epoch 3, mean loss 4.16\n",
      "Epoch 4, mean loss 4.04\n",
      "4 MAE: 21.89223304614566\n",
      "Epoch 0, mean loss 4.10\n",
      "Epoch 1, mean loss 4.13\n",
      "Epoch 2, mean loss 4.04\n",
      "Epoch 3, mean loss 4.18\n",
      "Epoch 4, mean loss 4.11\n",
      "4 MAE: 33.2182822351184\n",
      "Epoch 0, mean loss 4.09\n",
      "Epoch 1, mean loss 4.09\n",
      "Epoch 2, mean loss 4.02\n",
      "Epoch 3, mean loss 4.04\n",
      "Epoch 4, mean loss 4.17\n",
      "5 MAE: 22.38346320066855\n",
      "Epoch 0, mean loss 4.14\n",
      "Epoch 1, mean loss 4.16\n",
      "Epoch 2, mean loss 4.10\n",
      "Epoch 3, mean loss 4.09\n",
      "Epoch 4, mean loss 4.08\n",
      "5 MAE: 14.582838749632973\n",
      "Epoch 0, mean loss 4.11\n",
      "Epoch 1, mean loss 4.03\n",
      "Epoch 2, mean loss 4.08\n",
      "Epoch 3, mean loss 4.10\n",
      "Epoch 4, mean loss 4.05\n",
      "5 MAE: 16.38177443227044\n",
      "Epoch 0, mean loss 4.07\n",
      "Epoch 1, mean loss 4.15\n",
      "Epoch 2, mean loss 4.09\n",
      "Epoch 3, mean loss 3.97\n",
      "Epoch 4, mean loss 3.98\n",
      "5 MAE: 30.04054362344499\n",
      "Epoch 0, mean loss 4.09\n",
      "Epoch 1, mean loss 4.09\n",
      "Epoch 2, mean loss 3.99\n",
      "Epoch 3, mean loss 4.08\n",
      "Epoch 4, mean loss 4.03\n",
      "5 MAE: 16.80175821103786\n",
      "Epoch 0, mean loss 4.02\n",
      "Epoch 1, mean loss 4.07\n",
      "Epoch 2, mean loss 4.02\n",
      "Epoch 3, mean loss 4.11\n",
      "Epoch 4, mean loss 3.99\n",
      "5 MAE: 9.326868087591867\n",
      "Epoch 0, mean loss 4.03\n",
      "Epoch 1, mean loss 4.02\n",
      "Epoch 2, mean loss 4.06\n",
      "Epoch 3, mean loss 4.10\n",
      "Epoch 4, mean loss 4.14\n",
      "5 MAE: 17.009982424336037\n",
      "Epoch 0, mean loss 4.13\n",
      "Epoch 1, mean loss 4.03\n",
      "Epoch 2, mean loss 4.11\n",
      "Epoch 3, mean loss 4.04\n",
      "Epoch 4, mean loss 4.13\n",
      "5 MAE: 40.59926814443639\n",
      "Epoch 0, mean loss 4.13\n",
      "Epoch 1, mean loss 4.12\n",
      "Epoch 2, mean loss 4.05\n",
      "Epoch 3, mean loss 4.08\n",
      "Epoch 4, mean loss 4.04\n",
      "5 MAE: 21.154794122939084\n",
      "Epoch 0, mean loss 4.07\n",
      "Epoch 1, mean loss 4.06\n",
      "Epoch 2, mean loss 3.94\n",
      "Epoch 3, mean loss 4.02\n",
      "Epoch 4, mean loss 3.99\n",
      "5 MAE: 22.197967231972143\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "df_NN_2 = pd.DataFrame(columns = cols)\n",
    "NN_name = 'MLP20,4'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        train_file = os.path.join(baseDir, \"Training\", \"Features_Variant_\" + str(i) + \".csv\")\n",
    "        for j in range(1, 11):\n",
    "            t0_ = time.time()\n",
    "            test_file = os.path.join(baseDir, \"Testing/TestSet\", \"Test_Case_\" + str(j) + \".csv\")\n",
    "\n",
    "            x_train, y_train = ep.GetXYsFromFile(train_file)\n",
    "            x_test, y_test = ep.GetXYsFromFile(test_file)\n",
    "            scal = StandardScaler()\n",
    "            x_train = pd.DataFrame(scal.fit_transform(x_train))\n",
    "            x_test = pd.DataFrame(scal.transform(x_test))\n",
    "            \n",
    "            losses = []\n",
    "            n_epochs = 5\n",
    "            batch_size = 500\n",
    "            n_batches = len(x_train) // batch_size\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                for iteration in range(n_batches):\n",
    "                    X_batch, y_batch = ep.fetch_batch(x_train, y_train, epoch, iteration, batch_size)\n",
    "                    _, l, reg = sess.run([training_op, loss, regress], feed_dict={X: X_batch, y: y_batch})\n",
    "                    losses.append(l)\n",
    "                print(\"Epoch %i, mean loss %.2f\" % (epoch, np.mean(losses[-n_batches:])))\n",
    "\n",
    "            yPred = sess.run(regress, feed_dict = { X: x_test })\n",
    "            \n",
    "            # metrics\n",
    "            mae_val = mean_absolute_error(y_test, yPred) #mean_absolute_error(y_train, yPred)\n",
    "            print(i, \"MAE:\", mae_val)\n",
    "            y_pred_10, y_test_10 = ep.set_10(y_test,yPred)\n",
    "            dif = ep.hits_10(y_pred_10, y_test_10)\n",
    "            \n",
    "            AUC_10_ = ep.AUC(y_pred_10, y_test_10)\n",
    "            t1_ = time.time()\n",
    "            time_dif_ = t1_ - t0_\n",
    "            #print(dif_)\n",
    "            NN_score_2 = pd.Series({ \n",
    "                     'Regressor': 'MLP-20,4',\n",
    "                     'Training Set' : i,\n",
    "                     'Test Set': j,\n",
    "                     'Best Parameters':  'hidden layers:20,4',\n",
    "                     'Time Train': time_dif_,\n",
    "                     'HITS@10': dif,\n",
    "                     'AUC@10': AUC_10_,\n",
    "                     'MAE': mae_val},\n",
    "                )\n",
    "            \n",
    "            df_NN_2 = df_NN_2.append(NN_score_2, ignore_index = True)\n",
    "            \n",
    "# save output to file\n",
    "nowStr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "df_NN_2.to_csv(os.path.join(baseDir, \"%s-output-%s.csv\" % ('-'.join(NN_name), nowStr)))\n",
    "\n",
    "print(\"Done.\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We had evaluated the models with several metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from csv file all data (that were combined from previous files)\n",
    "df_scores = pd.DataFrame.from_csv(os.path.join(baseDir, \"Metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regressor</th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Test Set</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Time Train</th>\n",
       "      <th>HITS@10</th>\n",
       "      <th>AUC@10</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>123.128604</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>24.158418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>118.052021</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>21.377806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>113.489535</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>22.866102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>109.049385</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.320561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 1}</td>\n",
       "      <td>107.422607</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>18.540956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'fit_intercept': False, 'alpha': 0.5}</td>\n",
       "      <td>103.561349</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.167080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>116.881744</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>22.776141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>113.333616</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.813624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>113.311369</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.332932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>119.010635</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>28.574399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>364.924613</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.594389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>346.122898</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>24.644581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>326.578314</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24.301878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>341.658964</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>33.526867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>348.481484</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>19.256497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>342.642903</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.851703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>331.579003</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>22.861445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>358.417974</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>46.177839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>310.358455</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>26.559659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 10}</td>\n",
       "      <td>333.056584</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.254407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>593.422275</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>23.709665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>555.135755</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>21.742765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>555.240398</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>22.680379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>536.671889</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.250420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>547.914713</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>18.610478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>599.156383</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.493026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>567.118554</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>22.496414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>546.546465</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.787080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 0.5}</td>\n",
       "      <td>583.595260</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'fit_intercept': False, 'alpha': 0.5}</td>\n",
       "      <td>520.369316</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>27.510188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.067472</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>23.966404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.012185</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>15.297855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>3.990687</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.366409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>3.987776</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>29.773212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.077429</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15.837176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.037925</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>7.323620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.011431</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16.343165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>3.991107</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>38.304069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.025263</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>19.421785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>3.968470</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>31.698287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.456619</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>23.730544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.587255</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>15.085785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.497783</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.477413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.458585</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>30.640550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.494937</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15.483733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.447866</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8.683389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.514487</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16.334636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.461125</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>38.088920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.459728</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20.956230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.500127</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>27.464713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.972673</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>24.017576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.912216</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>14.640593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.893477</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16.075309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>5.080579</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>32.003059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.966866</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>30.710450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.927994</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8.367762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.909659</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16.403048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.895662</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40.747980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.981267</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>21.232296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>hidden layers:20,4</td>\n",
       "      <td>4.910255</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>74.402837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Regressor  Training Set  Test Set  \\\n",
       "0    Lasso Regression             1         1   \n",
       "1    Lasso Regression             1         2   \n",
       "2    Lasso Regression             1         3   \n",
       "3    Lasso Regression             1         4   \n",
       "4    Lasso Regression             1         5   \n",
       "5    Lasso Regression             1         6   \n",
       "6    Lasso Regression             1         7   \n",
       "7    Lasso Regression             1         8   \n",
       "8    Lasso Regression             1         9   \n",
       "9    Lasso Regression             1        10   \n",
       "10   Lasso Regression             2         1   \n",
       "11   Lasso Regression             2         2   \n",
       "12   Lasso Regression             2         3   \n",
       "13   Lasso Regression             2         4   \n",
       "14   Lasso Regression             2         5   \n",
       "15   Lasso Regression             2         6   \n",
       "16   Lasso Regression             2         7   \n",
       "17   Lasso Regression             2         8   \n",
       "18   Lasso Regression             2         9   \n",
       "19   Lasso Regression             2        10   \n",
       "20   Lasso Regression             3         1   \n",
       "21   Lasso Regression             3         2   \n",
       "22   Lasso Regression             3         3   \n",
       "23   Lasso Regression             3         4   \n",
       "24   Lasso Regression             3         5   \n",
       "25   Lasso Regression             3         6   \n",
       "26   Lasso Regression             3         7   \n",
       "27   Lasso Regression             3         8   \n",
       "28   Lasso Regression             3         9   \n",
       "29   Lasso Regression             3        10   \n",
       "..                ...           ...       ...   \n",
       "220          MLP-20,4             3         1   \n",
       "221          MLP-20,4             3         2   \n",
       "222          MLP-20,4             3         3   \n",
       "223          MLP-20,4             3         4   \n",
       "224          MLP-20,4             3         5   \n",
       "225          MLP-20,4             3         6   \n",
       "226          MLP-20,4             3         7   \n",
       "227          MLP-20,4             3         8   \n",
       "228          MLP-20,4             3         9   \n",
       "229          MLP-20,4             3        10   \n",
       "230          MLP-20,4             4         1   \n",
       "231          MLP-20,4             4         2   \n",
       "232          MLP-20,4             4         3   \n",
       "233          MLP-20,4             4         4   \n",
       "234          MLP-20,4             4         5   \n",
       "235          MLP-20,4             4         6   \n",
       "236          MLP-20,4             4         7   \n",
       "237          MLP-20,4             4         8   \n",
       "238          MLP-20,4             4         9   \n",
       "239          MLP-20,4             4        10   \n",
       "240          MLP-20,4             5         1   \n",
       "241          MLP-20,4             5         2   \n",
       "242          MLP-20,4             5         3   \n",
       "243          MLP-20,4             5         4   \n",
       "244          MLP-20,4             5         5   \n",
       "245          MLP-20,4             5         6   \n",
       "246          MLP-20,4             5         7   \n",
       "247          MLP-20,4             5         8   \n",
       "248          MLP-20,4             5         9   \n",
       "249          MLP-20,4             5        10   \n",
       "\n",
       "                            Best Parameters  Time Train  HITS@10  AUC@10  \\\n",
       "0     {'fit_intercept': True, 'alpha': 0.5}  123.128604        6     0.6   \n",
       "1     {'fit_intercept': True, 'alpha': 0.5}  118.052021        6     0.6   \n",
       "2     {'fit_intercept': True, 'alpha': 0.5}  113.489535        6     0.6   \n",
       "3     {'fit_intercept': True, 'alpha': 0.5}  109.049385        5     0.5   \n",
       "4       {'fit_intercept': True, 'alpha': 1}  107.422607        6     0.6   \n",
       "5    {'fit_intercept': False, 'alpha': 0.5}  103.561349        5     0.5   \n",
       "6     {'fit_intercept': True, 'alpha': 0.5}  116.881744        6     0.6   \n",
       "7     {'fit_intercept': True, 'alpha': 0.5}  113.333616        5     0.5   \n",
       "8     {'fit_intercept': True, 'alpha': 0.5}  113.311369        5     0.5   \n",
       "9     {'fit_intercept': True, 'alpha': 0.5}  119.010635        6     0.6   \n",
       "10     {'fit_intercept': True, 'alpha': 10}  364.924613        6     0.6   \n",
       "11     {'fit_intercept': True, 'alpha': 10}  346.122898        6     0.6   \n",
       "12     {'fit_intercept': True, 'alpha': 10}  326.578314        5     0.5   \n",
       "13     {'fit_intercept': True, 'alpha': 10}  341.658964        5     0.5   \n",
       "14     {'fit_intercept': True, 'alpha': 10}  348.481484        6     0.6   \n",
       "15     {'fit_intercept': True, 'alpha': 10}  342.642903        5     0.5   \n",
       "16     {'fit_intercept': True, 'alpha': 10}  331.579003        6     0.6   \n",
       "17     {'fit_intercept': True, 'alpha': 10}  358.417974        6     0.6   \n",
       "18     {'fit_intercept': True, 'alpha': 10}  310.358455        4     0.4   \n",
       "19     {'fit_intercept': True, 'alpha': 10}  333.056584        5     0.5   \n",
       "20    {'fit_intercept': True, 'alpha': 0.5}  593.422275        6     0.6   \n",
       "21    {'fit_intercept': True, 'alpha': 0.5}  555.135755        6     0.6   \n",
       "22    {'fit_intercept': True, 'alpha': 0.5}  555.240398        6     0.6   \n",
       "23    {'fit_intercept': True, 'alpha': 0.5}  536.671889        5     0.5   \n",
       "24    {'fit_intercept': True, 'alpha': 0.5}  547.914713        6     0.6   \n",
       "25    {'fit_intercept': True, 'alpha': 0.5}  599.156383        5     0.5   \n",
       "26    {'fit_intercept': True, 'alpha': 0.5}  567.118554        6     0.6   \n",
       "27    {'fit_intercept': True, 'alpha': 0.5}  546.546465        5     0.5   \n",
       "28    {'fit_intercept': True, 'alpha': 0.5}  583.595260        5     0.5   \n",
       "29   {'fit_intercept': False, 'alpha': 0.5}  520.369316        5     0.5   \n",
       "..                                      ...         ...      ...     ...   \n",
       "220                      hidden layers:20,4    4.067472        7     0.7   \n",
       "221                      hidden layers:20,4    4.012185        7     0.7   \n",
       "222                      hidden layers:20,4    3.990687        5     0.5   \n",
       "223                      hidden layers:20,4    3.987776        6     0.6   \n",
       "224                      hidden layers:20,4    4.077429        5     0.5   \n",
       "225                      hidden layers:20,4    4.037925        6     0.6   \n",
       "226                      hidden layers:20,4    4.011431        8     0.8   \n",
       "227                      hidden layers:20,4    3.991107        4     0.4   \n",
       "228                      hidden layers:20,4    4.025263        6     0.6   \n",
       "229                      hidden layers:20,4    3.968470        7     0.7   \n",
       "230                      hidden layers:20,4    4.456619        7     0.7   \n",
       "231                      hidden layers:20,4    4.587255        7     0.7   \n",
       "232                      hidden layers:20,4    4.497783        5     0.5   \n",
       "233                      hidden layers:20,4    4.458585        6     0.6   \n",
       "234                      hidden layers:20,4    4.494937        5     0.5   \n",
       "235                      hidden layers:20,4    4.447866        6     0.6   \n",
       "236                      hidden layers:20,4    4.514487        8     0.8   \n",
       "237                      hidden layers:20,4    4.461125        4     0.4   \n",
       "238                      hidden layers:20,4    4.459728        5     0.5   \n",
       "239                      hidden layers:20,4    4.500127        7     0.7   \n",
       "240                      hidden layers:20,4    4.972673        7     0.7   \n",
       "241                      hidden layers:20,4    4.912216        7     0.7   \n",
       "242                      hidden layers:20,4    4.893477        5     0.5   \n",
       "243                      hidden layers:20,4    5.080579        6     0.6   \n",
       "244                      hidden layers:20,4    4.966866        6     0.6   \n",
       "245                      hidden layers:20,4    4.927994        6     0.6   \n",
       "246                      hidden layers:20,4    4.909659        8     0.8   \n",
       "247                      hidden layers:20,4    4.895662        3     0.3   \n",
       "248                      hidden layers:20,4    4.981267        5     0.5   \n",
       "249                      hidden layers:20,4    4.910255        7     0.7   \n",
       "\n",
       "           MAE  \n",
       "0    24.158418  \n",
       "1    21.377806  \n",
       "2    22.866102  \n",
       "3    32.320561  \n",
       "4    18.540956  \n",
       "5    14.167080  \n",
       "6    22.776141  \n",
       "7    44.813624  \n",
       "8    26.332932  \n",
       "9    28.574399  \n",
       "10   25.594389  \n",
       "11   24.644581  \n",
       "12   24.301878  \n",
       "13   33.526867  \n",
       "14   19.256497  \n",
       "15   14.851703  \n",
       "16   22.861445  \n",
       "17   46.177839  \n",
       "18   26.559659  \n",
       "19   26.254407  \n",
       "20   23.709665  \n",
       "21   21.742765  \n",
       "22   22.680379  \n",
       "23   32.250420  \n",
       "24   18.610478  \n",
       "25   14.493026  \n",
       "26   22.496414  \n",
       "27   44.787080  \n",
       "28   26.943900  \n",
       "29   27.510188  \n",
       "..         ...  \n",
       "220  23.966404  \n",
       "221  15.297855  \n",
       "222  13.366409  \n",
       "223  29.773212  \n",
       "224  15.837176  \n",
       "225   7.323620  \n",
       "226  16.343165  \n",
       "227  38.304069  \n",
       "228  19.421785  \n",
       "229  31.698287  \n",
       "230  23.730544  \n",
       "231  15.085785  \n",
       "232  18.477413  \n",
       "233  30.640550  \n",
       "234  15.483733  \n",
       "235   8.683389  \n",
       "236  16.334636  \n",
       "237  38.088920  \n",
       "238  20.956230  \n",
       "239  27.464713  \n",
       "240  24.017576  \n",
       "241  14.640593  \n",
       "242  16.075309  \n",
       "243  32.003059  \n",
       "244  30.710450  \n",
       "245   8.367762  \n",
       "246  16.403048  \n",
       "247  40.747980  \n",
       "248  21.232296  \n",
       "249  74.402837  \n",
       "\n",
       "[250 rows x 8 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_lasso_1 = df_scores[(df_scores[\"Regressor\"] == 'Lasso Regression') & (df_scores['Training Set'] == 1)]\n",
    "df_lasso_2 = df_scores[(df_scores[\"Regressor\"] == 'Lasso Regression') & (df_scores['Training Set'] == 2)]\n",
    "df_lasso_3 = df_scores[(df_scores[\"Regressor\"] == 'Lasso Regression') & (df_scores['Training Set'] == 3)]\n",
    "df_lasso_4 = df_scores[(df_scores[\"Regressor\"] == 'Lasso Regression') & (df_scores['Training Set'] == 4)]\n",
    "df_lasso_5 = df_scores[(df_scores[\"Regressor\"] == 'Lasso Regression') & (df_scores['Training Set'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_lasso = pd.concat([df_lasso_1,df_lasso_2, df_lasso_3, df_lasso_4, df_lasso_5], ignore_index=True)\n",
    "#df_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tree_1 = df_scores[(df_scores[\"Regressor\"] == 'Decision Tree Regressor') & (df_scores['Training Set'] == 1)]\n",
    "df_tree_2 = df_scores[(df_scores[\"Regressor\"] == 'Decision Tree Regressor') & (df_scores['Training Set'] == 2)]\n",
    "df_tree_3 = df_scores[(df_scores[\"Regressor\"] == 'Decision Tree Regressor') & (df_scores['Training Set'] == 3)]\n",
    "df_tree_4 = df_scores[(df_scores[\"Regressor\"] == 'Decision Tree Regressor') & (df_scores['Training Set'] == 4)]\n",
    "df_tree_5 = df_scores[(df_scores[\"Regressor\"] == 'Decision Tree Regressor') & (df_scores['Training Set'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tree = pd.concat([df_tree_1,df_tree_2, df_tree_3, df_tree_4, df_tree_5], ignore_index=True)\n",
    "#df_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rf_1 = df_scores[(df_scores[\"Regressor\"] == 'Random Forest Regressor') & (df_scores['Training Set'] == 1)]\n",
    "df_rf_2 = df_scores[(df_scores[\"Regressor\"] == 'Random Forest Regressor') & (df_scores['Training Set'] == 2)]\n",
    "df_rf_3 = df_scores[(df_scores[\"Regressor\"] == 'Random Forest Regressor') & (df_scores['Training Set'] == 3)]\n",
    "df_rf_4 = df_scores[(df_scores[\"Regressor\"] == 'Random Forest Regressor') & (df_scores['Training Set'] == 4)]\n",
    "df_rf_5 = df_scores[(df_scores[\"Regressor\"] == 'Random Forest Regressor') & (df_scores['Training Set'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rf = pd.concat([df_rf_1,df_rf_2, df_rf_3, df_rf_4, df_rf_5], ignore_index=True)\n",
    "#df_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_MLP1_1 = df_scores[(df_scores[\"Regressor\"] == 'MLP-4') & (df_scores['Training Set'] == 1)]\n",
    "df_MLP1_2 = df_scores[(df_scores[\"Regressor\"] == 'MLP-4') & (df_scores['Training Set'] == 2)]\n",
    "df_MLP1_3 = df_scores[(df_scores[\"Regressor\"] == 'MLP-4') & (df_scores['Training Set'] == 3)]\n",
    "df_MLP1_4 = df_scores[(df_scores[\"Regressor\"] == 'MLP-4') & (df_scores['Training Set'] == 4)]\n",
    "df_MLP1_5 = df_scores[(df_scores[\"Regressor\"] == 'MLP-4') & (df_scores['Training Set'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_MLP1 = pd.concat([df_MLP1_1,df_MLP1_2, df_MLP1_3, df_MLP1_4, df_MLP1_5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_MLP2_1 = df_scores[(df_scores[\"Regressor\"] == 'MLP-20,4') & (df_scores['Training Set'] == 1)]\n",
    "df_MLP2_2 = df_scores[(df_scores[\"Regressor\"] == 'MLP-20,4') & (df_scores['Training Set'] == 2)]\n",
    "df_MLP2_3 = df_scores[(df_scores[\"Regressor\"] == 'MLP-20,4') & (df_scores['Training Set'] == 3)]\n",
    "df_MLP2_4 = df_scores[(df_scores[\"Regressor\"] == 'MLP-20,4') & (df_scores['Training Set'] == 4)]\n",
    "df_MLP2_5 = df_scores[(df_scores[\"Regressor\"] == 'MLP-20,4') & (df_scores['Training Set'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_MLP2 = pd.concat([df_MLP2_1,df_MLP2_2, df_MLP2_3, df_MLP2_4, df_MLP2_5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regressor</th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Time Train</th>\n",
       "      <th>HITS@10</th>\n",
       "      <th>AUC@10</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1</td>\n",
       "      <td>113.724087</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.56</td>\n",
       "      <td>25.592802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>2</td>\n",
       "      <td>340.382119</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.54</td>\n",
       "      <td>26.402927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>3</td>\n",
       "      <td>560.517101</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>25.522431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>4</td>\n",
       "      <td>780.887137</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>26.181665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>5</td>\n",
       "      <td>1097.196695</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.56</td>\n",
       "      <td>25.187853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>1</td>\n",
       "      <td>16.659041</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.67</td>\n",
       "      <td>22.423141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>2</td>\n",
       "      <td>29.101371</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>23.034941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>3</td>\n",
       "      <td>38.210702</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.63</td>\n",
       "      <td>24.106209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>4</td>\n",
       "      <td>47.078026</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.66</td>\n",
       "      <td>21.163361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>5</td>\n",
       "      <td>56.256792</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>22.264735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>1</td>\n",
       "      <td>5.689706</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.67</td>\n",
       "      <td>22.369494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>2</td>\n",
       "      <td>11.421712</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.67</td>\n",
       "      <td>21.353054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>3</td>\n",
       "      <td>18.880280</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.68</td>\n",
       "      <td>21.908231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>4</td>\n",
       "      <td>27.410310</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.69</td>\n",
       "      <td>21.116305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Random Forest Regressor</td>\n",
       "      <td>5</td>\n",
       "      <td>31.378967</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.68</td>\n",
       "      <td>21.696875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP-4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.758668</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>22.285309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP-4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.339164</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>20.899950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP-4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.725371</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.59</td>\n",
       "      <td>23.707005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP-4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.294625</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.57</td>\n",
       "      <td>29.991285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP-4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.798874</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.63</td>\n",
       "      <td>27.962354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.161715</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>23.461452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.576391</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.62</td>\n",
       "      <td>20.735821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.016974</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>21.133198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.487851</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>21.494591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP-20,4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.945065</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>27.860091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Regressor Training Set   Time Train  HITS@10  AUC@10  \\\n",
       "0          Lasso Regression            1   113.724087      5.6    0.56   \n",
       "1          Lasso Regression            2   340.382119      5.4    0.54   \n",
       "2          Lasso Regression            3   560.517101      5.5    0.55   \n",
       "3          Lasso Regression            4   780.887137      5.5    0.55   \n",
       "4          Lasso Regression            5  1097.196695      5.6    0.56   \n",
       "5   Decision Tree Regressor            1    16.659041      6.7    0.67   \n",
       "6   Decision Tree Regressor            2    29.101371      6.5    0.65   \n",
       "7   Decision Tree Regressor            3    38.210702      6.3    0.63   \n",
       "8   Decision Tree Regressor            4    47.078026      6.6    0.66   \n",
       "9   Decision Tree Regressor            5    56.256792      6.1    0.61   \n",
       "10  Random Forest Regressor            1     5.689706      6.7    0.67   \n",
       "11  Random Forest Regressor            2    11.421712      6.7    0.67   \n",
       "12  Random Forest Regressor            3    18.880280      6.8    0.68   \n",
       "13  Random Forest Regressor            4    27.410310      6.9    0.69   \n",
       "14  Random Forest Regressor            5    31.378967      6.8    0.68   \n",
       "15                    MLP-4            1     3.758668      6.0    0.60   \n",
       "16                    MLP-4            2     4.339164      6.0    0.60   \n",
       "17                    MLP-4            3     4.725371      5.9    0.59   \n",
       "18                    MLP-4            4     5.294625      5.7    0.57   \n",
       "19                    MLP-4            5     5.798874      6.3    0.63   \n",
       "20                 MLP-20,4            1     3.161715      6.1    0.61   \n",
       "21                 MLP-20,4            2     3.576391      6.2    0.62   \n",
       "22                 MLP-20,4            3     4.016974      6.1    0.61   \n",
       "23                 MLP-20,4            4     4.487851      6.0    0.60   \n",
       "24                 MLP-20,4            5     4.945065      6.0    0.60   \n",
       "\n",
       "          MAE  \n",
       "0   25.592802  \n",
       "1   26.402927  \n",
       "2   25.522431  \n",
       "3   26.181665  \n",
       "4   25.187853  \n",
       "5   22.423141  \n",
       "6   23.034941  \n",
       "7   24.106209  \n",
       "8   21.163361  \n",
       "9   22.264735  \n",
       "10  22.369494  \n",
       "11  21.353054  \n",
       "12  21.908231  \n",
       "13  21.116305  \n",
       "14  21.696875  \n",
       "15  22.285309  \n",
       "16  20.899950  \n",
       "17  23.707005  \n",
       "18  29.991285  \n",
       "19  27.962354  \n",
       "20  23.461452  \n",
       "21  20.735821  \n",
       "22  21.133198  \n",
       "23  21.494591  \n",
       "24  27.860091  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_ = ['Regressor','Training Set', 'Time Train', 'HITS@10', 'AUC@10', 'MAE']\n",
    "df = pd.DataFrame(columns = cols_)\n",
    "\n",
    "metric = ['Time Train', 'HITS@10', 'AUC@10', 'MAE']\n",
    "reg_list = ['Lasso Regression', 'Decision Tree Regressor', 'Random Forest Regressor','MLP-4','MLP-20,4']\n",
    "for r in range(len(reg_list)):\n",
    "    for n in range(1,6):\n",
    "        df_ = df_scores[(df_scores[\"Regressor\"] == reg_list[r]) & (df_scores['Training Set'] == n)]\n",
    "        df_sc = pd.Series({ \n",
    "                     'Regressor': reg_list[r],\n",
    "                     'Training Set' : n,\n",
    "                     'Time Train': df_['Time Train'].mean(),\n",
    "                     'HITS@10':df_['HITS@10'].mean(),\n",
    "                     'AUC@10': df_['AUC@10'].mean(),\n",
    "                     'MAE': df_['MAE'].mean()},\n",
    "                )\n",
    "        df = df.append(df_sc, ignore_index = True)\n",
    "df                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(metric,name):\n",
    "    n_groups = 5\n",
    "\n",
    "    var_1 = (df_lasso_1[metric].mean(), df_tree_1[metric].mean(), df_rf_1[metric].mean(),df_MLP1_1[metric].mean(),df_MLP2_1[metric].mean())\n",
    "    var_2 = (df_lasso_2[metric].mean(), df_tree_2[metric].mean(), df_rf_2[metric].mean(),df_MLP1_2[metric].mean(),df_MLP2_2[metric].mean())\n",
    "    var_3 = (df_lasso_3[metric].mean(), df_tree_3[metric].mean(), df_rf_3[metric].mean(),df_MLP1_3[metric].mean(),df_MLP2_3[metric].mean())\n",
    "    var_4 = (df_lasso_4[metric].mean(), df_tree_4[metric].mean(), df_rf_4[metric].mean(),df_MLP1_4[metric].mean(),df_MLP2_4[metric].mean())\n",
    "    var_5 = (df_lasso_5[metric].mean(), df_tree_5[metric].mean(), df_rf_5[metric].mean(),df_MLP1_5[metric].mean(),df_MLP2_5[metric].mean())\n",
    " \n",
    "    # create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.15\n",
    "    opacity = 0.8\n",
    "    \n",
    "    rects1 = plt.bar(index, var_1, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     label='Variant 1')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, var_2, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='g',\n",
    "                     label='Variant 2')\n",
    "\n",
    "    rects3 = plt.bar(index + bar_width*2, var_3, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     label='Variant 3')\n",
    "    rects4 = plt.bar(index + bar_width*3, var_4, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='y',\n",
    "                     label='Variant 4')\n",
    "    rects5 = plt.bar(index + bar_width*4, var_5, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='m',\n",
    "                     label='Variant 5')\n",
    "\n",
    "    plt.title(name)\n",
    "    plt.xticks(index + bar_width, ('LassoReg', 'DecTree', 'RandForest','MLP-4','MLP-20,4'))\n",
    "    plt.legend()\n",
    " \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare average evaluation time for all 5 variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso time train 578.54142771\n",
      "Decision tree time train 37.4611863234\n",
      "RandomForest time train 18.95619520594\n",
      "MLP(4) time train 4.783340268059999\n",
      "MLP(20,4) time train 4.037599329920002\n"
     ]
    }
   ],
   "source": [
    "print('Lasso time train',df_lasso['Time Train'].mean())\n",
    "print('Decision tree time train',df_tree['Time Train'].mean())\n",
    "print('RandomForest time train',df_rf['Time Train'].mean())\n",
    "print('MLP(4) time train',df_MLP1['Time Train'].mean())\n",
    "print('MLP(20,4) time train',df_MLP2['Time Train'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measure includes the time to train the regressor . From the graph we can see that in the average the  MLP (20,4) - 4.03 sec and MLP(4) - 4.78 sec had predicted in minimal time and followed by Random Forest 18.96 sec. The Lasso Regression had taken the highest prediction time 578.54 in average (1097.20 sec in variant 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmWXJAcrMzLLkAGVmZln6/4CMWdPx9dTkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13449b6b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot('Time Train', 'Evaluation Time (sec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITS@10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare average HITS@10 for all 5 variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso HITS@10 5.52\n",
      "Decision tree HITS@10 6.44\n",
      "RandomForest HITS@10 6.78\n",
      "MLP(4) HITS@10 5.98\n",
      "MLP(20,4) HITS@10 6.08\n"
     ]
    }
   ],
   "source": [
    "print('Lasso HITS@10',df_lasso['HITS@10'].mean())\n",
    "print('Decision tree HITS@10',df_tree['HITS@10'].mean())\n",
    "print('RandomForest HITS@10',df_rf['HITS@10'].mean())\n",
    "print('MLP(4) HITS@10',df_MLP1['HITS@10'].mean())\n",
    "print('MLP(20,4) HITS@10',df_MLP2['HITS@10'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HITS@10 describes prediction accuracy of the model. The chart shows that the Random Forest (6.78) regressor achieves the prediction accuracy, closely followed by Decision Tree (6.44)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1344acc80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot('HITS@10', 'Average Hits@10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC@10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare average AUC@10 for all 5 variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso AUC@10 0.5520000000000002\n",
      "Decision tree AUC@10 0.644\n",
      "RandomForest AUC@10 0.6780000000000003\n",
      "MLP(4) AUC@10 0.5980000000000003\n",
      "MLP(20,4) AUC@10 0.608\n"
     ]
    }
   ],
   "source": [
    "print('Lasso AUC@10',df_lasso['AUC@10'].mean())\n",
    "print('Decision tree AUC@10',df_tree['AUC@10'].mean())\n",
    "print('RandomForest AUC@10',df_rf['AUC@10'].mean())\n",
    "print('MLP(4) AUC@10',df_MLP1['AUC@10'].mean())\n",
    "print('MLP(20,4) AUC@10',df_MLP2['AUC@10'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Singh et al. (2015)'s paper, AUC@10 is calculated as:\n",
    "$$\\frac{Tp}{Tp+Fp} $$\n",
    "\n",
    "Although the paper does not explain what is meant by true positives $Tp$ and false positives $Fp$, I understood it as follows. Given the top 10 posts (by the number of comments) predicted by the model, y_pred_10, and top 10 true posts, y_test_10:\n",
    "\n",
    "* $Tp$ is the number of posts in y_pred_10 that are also in y_test_10: posts that the model __correctly__ predicted as being in top 10\n",
    "* $Fp$ is the number of posts in y_pred_10 that are not in y_test_10: posts that the model __wrongly__ predicted as being in top 10\n",
    "It shows the percentage of true positives among predicted values in the top 10 posts by the number of comments.\n",
    "\n",
    "Here, as with HITS@10, Random Forest performed best and showed the highest percentage of correctly predicted values in the top 10, again closely followed by Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDMgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1344adc7ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot('AUC@10', 'Average AUC@10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE\n",
    "\n",
    "Compare average MAE for all 5 variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso MAE 25.777535540800002\n",
      "Decision tree MAE 22.598477213599995\n",
      "RandomForest MAE 21.688791913799992\n",
      "MLP(4) MAE 24.969180668499998\n",
      "MLP(20,4) MAE 22.93703051493999\n"
     ]
    }
   ],
   "source": [
    "print('Lasso MAE',df_lasso['MAE'].mean())\n",
    "print('Decision tree MAE',df_tree['MAE'].mean())\n",
    "print('RandomForest MAE',df_rf['MAE'].mean())\n",
    "print('MLP(4) MAE',df_MLP1['MAE'].mean())\n",
    "print('MLP(20,4) MAE',df_MLP2['MAE'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE stands for Mean Absolute Error of prediction by the models. Accordingly, lower scores indicate smaller errors and better results. The chart below shows that Random Forest has the lowest average error - 21.69. The next is Decision Tree - 22.60 and MLP(20,4) - 22.93 The highest error was observed in Lasso Regression - 25.78 and in MLP-4 and MLP-20,4 in variants 4 and 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-4, variant 4: 29.991285334000004\n",
      "MLP-4, variant 5: 27.9623539979\n",
      "MLP-20,4, variant 4: 27.860090931500004\n"
     ]
    }
   ],
   "source": [
    "print('MLP-4, variant 4:',df_MLP1_4['MAE'].mean())\n",
    "print('MLP-4, variant 5:',df_MLP1_5['MAE'].mean())\n",
    "print('MLP-20,4, variant 4:',df_MLP2_5['MAE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x134005aa3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot('MAE', 'Average MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "This project revisited the results of the paper by Singh et al. (2015). I re-ran three methods that were employed by Singh et al.: Decision Tree, MLP-4 and MLP-20,4. Similarly to conclusions reached in that paper, I observed that Decision Tree produces more accurate predictions that both neural networks, MLP-4 and MLP-20,4.\n",
    "\n",
    "In addition, using knowledge obtained during this course, I used Lasso Regression and Random Forest. The results show that Random Forest produces even highest accuracy than Decision Tree across all accuracy measures. It has also greater consistency across the 5 training sets: accuracy measures show smaller variance across the training sets compared to other regressors (see, e.g., the MAE chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
